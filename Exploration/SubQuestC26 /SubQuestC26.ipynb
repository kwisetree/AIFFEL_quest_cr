{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmTo/R4gzmiZDZWr+g093w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwisetree/AIFFEL_quest_cr/blob/main/Exploration/SubQuestC26%20/SubQuestC26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#프로젝트: 한국어 데이터로 챗봇 만들기"
      ],
      "metadata": {
        "id": "JMAJZwKb68Fj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQLDxvQf6eRU",
        "outputId": "cd144316-e6e6-44de-f475-264f6773dbed",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1. 데이터 수집하기"
      ],
      "metadata": {
        "id": "qm_2l2-17u-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 폴더 만들기\n",
        "data_dir = '/content/aiffel/transformer_chatbot/data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# GitHub에서 직접 읽고 저장\n",
        "url = 'https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv'\n",
        "save_path = os.path.join(data_dir, 'ChatbotData.csv')\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df.to_csv(save_path, index=False)\n",
        "\n",
        "print(f\"데이터 저장 완료: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfMSAy4Y9JYD",
        "outputId": "fb529df3-e10b-4191-9d99-ade7c0392b3d",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 저장 완료: /content/aiffel/transformer_chatbot/data/ChatbotData.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 경로에서 다시 불러오기\n",
        "df_check = pd.read_csv(save_path)\n",
        "df_check.head()"
      ],
      "metadata": {
        "id": "PGbYeEAJ7BNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2. 데이터 전처리하기\n",
        "---\n",
        "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n"
      ],
      "metadata": {
        "id": "TZQcSsCw8FVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "clMifUKb8MsK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수: 간단한 특수문자 제거 + 공백 정리 (숫자 유지)\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = str(sentence).strip().lower()  # 소문자화 + 양쪽 공백 제거\n",
        "\n",
        "    # 구두점(punctuation) 주변에 공백 추가\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "\n",
        "    # 여러 공백은 하나로\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    # 한글, 영어, 숫자, 주요 구두점만 남기고 나머지 제거\n",
        "    sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9?.!,]+\", \" \", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "fE9Fzond_mWy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 대답 각각 전처리 적용\n",
        "questions = [preprocess_sentence(q) for q in df['Q']]\n",
        "answers = [preprocess_sentence(a) for a in df['A']]\n",
        "\n",
        "# 확인\n",
        "for i in range(5):\n",
        "    print(f\"Q: {questions[i]}\")\n",
        "    print(f\"A: {answers[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trH_LL0MihS2",
        "outputId": "556c85b6-5a3b-4927-f266-8541388dd0b2",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: 12시 땡 !\n",
            "A: 하루가 또 가네요 .\n",
            "\n",
            "Q: 1지망 학교 떨어졌어\n",
            "A: 위로해 드립니다 .\n",
            "\n",
            "Q: 3박4일 놀러가고 싶다\n",
            "A: 여행은 언제나 좋죠 .\n",
            "\n",
            "Q: 3박4일 정도 놀러가고 싶다\n",
            "A: 여행은 언제나 좋죠 .\n",
            "\n",
            "Q: ppl 심하네\n",
            "A: 눈살이 찌푸려지죠 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3. SubwordTextEncoder 사용하기\n",
        "---\n",
        "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요.\n"
      ],
      "metadata": {
        "id": "zhJhDkba8NJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 문장을 하나로 합쳐 단어장 생성용 코퍼스 만들기\n",
        "corpus = questions + answers"
      ],
      "metadata": {
        "id": "wsdhClPu8Swl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SubwordTextEncoder 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus, target_vocab_size=2**14)\n",
        "\n",
        "# 시작 토큰과 종료 토큰의 ID\n",
        "START_TOKEN_ID = tokenizer.vocab_size\n",
        "END_TOKEN_ID = tokenizer.vocab_size + 1\n",
        "\n",
        "# 단어장 크기 (추가 토큰 포함)\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "\n",
        "print(f'단어장 크기: {VOCAB_SIZE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEoI_c0ckZYe",
        "outputId": "660e42a9-5da3-4952-8f75-87e6094dcbd0",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장 크기: 21836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장을 토큰화하는 함수\n",
        "def tokenize_and_filter(inputs, outputs, max_length=40):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (input_sentence, output_sentence) in zip(inputs, outputs):\n",
        "        # 각 문장을 정수 시퀀스로 변환\n",
        "        input_sentence_tokens = [START_TOKEN_ID] + tokenizer.encode(input_sentence) + [END_TOKEN_ID]\n",
        "        output_sentence_tokens = [START_TOKEN_ID] + tokenizer.encode(output_sentence) + [END_TOKEN_ID]\n",
        "\n",
        "        # 최대 길이 체크\n",
        "        if len(input_sentence_tokens) <= max_length and len(output_sentence_tokens) <= max_length:\n",
        "            tokenized_inputs.append(input_sentence_tokens)\n",
        "            tokenized_outputs.append(output_sentence_tokens)\n",
        "\n",
        "    # 패딩 추가\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=max_length, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=max_length, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs"
      ],
      "metadata": {
        "id": "Ii_TlHnMpmdX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 및 필터링 적용\n",
        "MAX_LENGTH = 40\n",
        "questions_tokens, answers_tokens = tokenize_and_filter(questions, answers, MAX_LENGTH)\n",
        "\n",
        "print(f'질문 토큰화 결과 크기: {len(questions_tokens)}')\n",
        "print(f'대답 토큰화 결과 크기: {len(answers_tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLhHKP0qpplc",
        "outputId": "f820c424-5723-41d3-fcbc-773b77e0bc5c",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 토큰화 결과 크기: 11823\n",
            "대답 토큰화 결과 크기: 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 생성\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더 입력과 타겟 구분 (교사 강요를 위한 준비)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions_tokens,\n",
        "        'dec_inputs': answers_tokens[:, :-1]  # 마지막 토큰 제외\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers_tokens[:, 1:]  # 첫 번째 토큰 제외\n",
        "    }\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(f'데이터셋: {dataset}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb7WqtCqkeRD",
        "outputId": "9a49b574-c1f0-4537-ea37-32a321b1169e",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터셋: <_PrefetchDataset element_spec=({'inputs': TensorSpec(shape=(None, 40), dtype=tf.int32, name=None), 'dec_inputs': TensorSpec(shape=(None, 39), dtype=tf.int32, name=None)}, {'outputs': TensorSpec(shape=(None, 39), dtype=tf.int32, name=None)})>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4. 모델 구성하기\n",
        "---\n",
        "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
      ],
      "metadata": {
        "id": "vzb9YkGi8TXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 포지셔널 인코딩"
      ],
      "metadata": {
        "id": "B5O-KY2clLdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위치 인코딩\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # sin 함수는 짝수 인덱스에 적용\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # cos 함수는 홀수 인덱스에 적용\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "AHiFjsnq8YAK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 스케일드 닷 프로덕트 어텐션"
      ],
      "metadata": {
        "id": "rwWOWrv_lOrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 어텐션 마스킹 함수\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "metadata": {
        "id": "DGJQvMYkY8Wm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일드 닷 프로덕트 어텐션\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    # Q와 K의 곱셈 연산\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # 스케일링\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # 마스킹 적용\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # 소프트맥스로 어텐션 가중치 계산\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # 가중치와 V 곱하기\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "Nurqktz2lRfN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 멀티헤드 어텐션"
      ],
      "metadata": {
        "id": "e6vD0-bklT5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 멀티헤드 어텐션 레이어\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # 스케일드 닷 프로덕트 어텐션 적용\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "AOgGlMTQlWeM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 포인트 와이즈 피드포워드 네트워크"
      ],
      "metadata": {
        "id": "848zDCaLlYtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 포지션 와이즈 피드 포워드 네트워크\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "0Z11XmB8laiN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 인코더 레이어"
      ],
      "metadata": {
        "id": "egodVp-4lgfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 레이어\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(v=x, k=x, q=x, mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "sUczSz8LlikD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 디코더 레이어"
      ],
      "metadata": {
        "id": "2UcLBP0XlkwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 레이어\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(v=x, k=x, q=x, mask=look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            v=enc_output, k=enc_output, q=out1, mask=padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "ILNUMAUwlmlq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 전체 인코더"
      ],
      "metadata": {
        "id": "u-UO1CtTlrd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                              self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                         for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # 입력을 임베딩\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x=x, training=training, mask=mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "y9flu4bSlqzU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 전체 디코더"
      ],
      "metadata": {
        "id": "T1qtJjdHl7cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                         for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](\n",
        "                x=x,\n",
        "                enc_output=enc_output,\n",
        "                training=training,\n",
        "                look_ahead_mask=look_ahead_mask,\n",
        "                padding_mask=padding_mask\n",
        "            )\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "JPReaRQcl8Cj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. 트랜스포머 전체 모델"
      ],
      "metadata": {
        "id": "G3GACNfCl8b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 마스크 생성: 입력 시퀀스에서 패딩 토큰(0)에 해당하는 위치를 마스킹\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (배치 크기, 1, 1, 시퀀스 길이)\n",
        "\n",
        "# 미래 토큰 마스킹 (Look-ahead mask): 디코더가 미래의 토큰을 보지 못하게 막음\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (시퀀스 길이, 시퀀스 길이)\n",
        "\n",
        "# 전체 마스크 생성 함수: 인코더/디코더 패딩 마스크와 Look-ahead 마스크 결합\n",
        "def create_masks(inp, tar):\n",
        "    # 인코더 입력에 대한 패딩 마스크\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # 디코더 입력에 대한 look-ahead 마스크와 패딩 마스크 결합\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    # 디코더의 인코더 출력에 적용할 패딩 마스크\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "metadata": {
        "id": "TwSGTxAnc0AU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머 모델\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                             target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Keras 모델은 inputs를 리스트나 딕셔너리로 받음\n",
        "        inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "\n",
        "        enc_output = self.encoder(\n",
        "            x=inp,\n",
        "            training=training,\n",
        "            mask=enc_padding_mask\n",
        "        )\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            x=tar,\n",
        "            enc_output=enc_output,\n",
        "            training=training,\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "in5pO5WOl_kQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. 모델 생성"
      ],
      "metadata": {
        "id": "5b_cJ1BMqisH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 파라미터 설정\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=VOCAB_SIZE,\n",
        "    target_vocab_size=VOCAB_SIZE,\n",
        "    pe_input=MAX_LENGTH,\n",
        "    pe_target=MAX_LENGTH,\n",
        "    rate=dropout_rate\n",
        ")"
      ],
      "metadata": {
        "id": "zMB56XOcqiBi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 정의\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        real, pred, from_logits=True)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "# 정확도 계산 함수 정의\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "nKv3tD_oZiup"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률 스케줄러 정의\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # step을 float32로 명시적 변환 추가\n",
        "        step = tf.cast(step, tf.float32)\n",
        "\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# 학습률 설정 및 옵티마이저 정의\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# 체크포인트 설정\n",
        "checkpoint_path = './checkpoints/transformer'\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "metadata": {
        "id": "kyGMiHIVqntU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수 정의\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                'inputs': inp,\n",
        "                'dec_inputs': tar_inp\n",
        "            },\n",
        "            training=True\n",
        "        )\n",
        "\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "3BNSxnyQZxVi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tqdm 라이브러리 설치 및 가져오기\n",
        "!pip install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# 학습 메트릭 정의\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "# 모델 학습\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    print(f\"에포크 {epoch + 1}/{EPOCHS} 시작...\")\n",
        "\n",
        "    # tqdm으로 배치 순회를 감싸서 진행 표시줄 표시\n",
        "    for (batch, (inp, tar)) in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "        train_step(inp['inputs'], tar['outputs'])\n",
        "\n",
        "    # 에포크 완료 후 결과 출력\n",
        "    epoch_time = time.time() - start\n",
        "    print(f'에포크 {epoch + 1} 완료 - 손실: {train_loss.result():.4f}, 정확도: {train_accuracy.result():.4f}, 시간: {epoch_time:.2f}초')\n",
        "\n",
        "    # 에포크가 끝날 때마다 체크포인트 저장\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'에포크 {epoch + 1}에 체크포인트 저장: {ckpt_save_path}')\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "TVWRRqH5ZzFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 복원\n",
        "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    print(f\"체크포인트에서 복원됨: {ckpt_manager.latest_checkpoint}\")\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "else:\n",
        "    print(\"체크포인트가 없습니다. 처음부터 학습을 시작합니다.\")\n",
        "    start_epoch = 0\n",
        "\n",
        "# 이어서 학습 (start_epoch부터 100까지)\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    print(f\"에포크 {epoch + 1}/{EPOCHS} 시작...\")\n",
        "\n",
        "    for (batch, (inp, tar)) in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "        train_step(inp['inputs'], tar['outputs'])\n",
        "\n",
        "    epoch_time = time.time() - start\n",
        "    print(f'에포크 {epoch + 1} 완료 - 손실: {train_loss.result():.4f}, 정확도: {train_accuracy.result():.4f}, 시간: {epoch_time:.2f}초')\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'에포크 {epoch + 1}에 체크포인트 저장: {ckpt_save_path}')\n",
        "\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "Avvd2-W2xzsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5. 모델 평가하기\n",
        "---\n",
        "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
      ],
      "metadata": {
        "id": "St4VUDd_8aqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 체크포인트 로드 (학습 이후)\n",
        "# 저장된 체크포인트가 있으면 최신 체크포인트를 복원\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(f'최신 체크포인트 복원: {ckpt_manager.latest_checkpoint}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HNAmThxfnd91",
        "outputId": "11ee4171-eebf-4d5c-a168-2e5db0fedc10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최신 체크포인트 복원: ./checkpoints/transformer/ckpt-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 응답 생성 함수 정의\n",
        "def evaluate(sentence):\n",
        "    # 문장 전처리\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # 문장 토큰화\n",
        "    encoder_input = tokenizer.encode(sentence)\n",
        "    encoder_input = [START_TOKEN_ID] + encoder_input + [END_TOKEN_ID]\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # 디코더 입력은 시작 토큰으로 시작\n",
        "    decoder_input = [START_TOKEN_ID]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    # 자동 회귀(auto-regressive) 방식으로 출력 생성\n",
        "    for i in range(MAX_LENGTH):\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                'inputs': encoder_input,\n",
        "                'dec_inputs': output\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        # 마지막 토큰의 예측에서 다음 토큰 선택\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)  # int32로 명시적 변환\n",
        "\n",
        "        # 종료 토큰이 예측되면 반환\n",
        "        if tf.equal(predicted_id, END_TOKEN_ID):  # tf.equal 사용\n",
        "            break\n",
        "\n",
        "        # 예측된 ID를 디코더 입력에 연결\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # 출력 텍스트로 변환\n",
        "    output_text = tokenizer.decode([i for i in output[0].numpy() if i < tokenizer.vocab_size])\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "kHM5y46U8h4_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇 응답 함수\n",
        "def chat_response(sentence):\n",
        "    result = evaluate(sentence)\n",
        "\n",
        "    # 특수 토큰 제거 및 정리\n",
        "    result = result.replace('<start>', '').replace('<end>', '').strip()\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "3h7iVw_Gmi4g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 몇 가지 질문에 대한 응답 테스트\n",
        "test_questions = [\n",
        "    '안녕하세요?',\n",
        "    '오늘 날씨가 어때요?',\n",
        "    '당신의 이름은 뭐예요?',\n",
        "    '취미가 뭐예요?',\n",
        "    '한국어 잘하시네요'\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print('질문:', question)\n",
        "    print('응답:', chat_response(question))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9NjqvVgmlEI",
        "outputId": "846b6574-067f-47d1-d420-430d80f44bac",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: 안녕하세요?\n",
            "응답: 잘 하셨어요 .\n",
            "\n",
            "질문: 오늘 날씨가 어때요?\n",
            "응답: 더 힘들어져요 .\n",
            "\n",
            "질문: 당신의 이름은 뭐예요?\n",
            "응답: 더 힘들어져요 .\n",
            "\n",
            "질문: 취미가 뭐예요?\n",
            "응답: 잘 알 먹어보세요 .\n",
            "\n",
            "질문: 한국어 잘하시네요\n",
            "응답: 더 힘들어져요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화형 챗봇 인터페이스\n",
        "def interactive_chat():\n",
        "    print(\"한국어 챗봇과 대화를 시작합니다. '종료'를 입력하면 대화가 종료됩니다.\")\n",
        "    print('=' * 80)\n",
        "\n",
        "    while True:\n",
        "        user_input = input('사용자 > ')\n",
        "\n",
        "        if user_input.lower() == '종료':\n",
        "            print('챗봇과의 대화를 종료합니다.')\n",
        "            break\n",
        "\n",
        "        response = chat_response(user_input)\n",
        "        print('챗봇 >', response)\n",
        "        print()"
      ],
      "metadata": {
        "id": "K9GGabLVoixH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화형 인터페이스 실행\n",
        "interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6Qq2wt18okx_",
        "outputId": "cbcebbc7-645b-49f3-cee4-0e14a7d03cff"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 챗봇과 대화를 시작합니다. '종료'를 입력하면 대화가 종료됩니다.\n",
            "================================================================================\n",
            "사용자 > 안녕\n",
            "챗봇 > 마세요 .\n",
            "\n",
            "사용자 > 뭘 말아?\n",
            "챗봇 > 잘 하셨어요 .\n",
            "\n",
            "사용자 > 뭘 잘해?\n",
            "챗봇 > 잘 견디고 있네요 .\n",
            "\n",
            "사용자 > 뭘 견디는데?\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 너가 힘든 건 아니고?\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 왜 힘드냐고\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 왜 더 힘든지 말해주겠니?\n",
            "챗봇 > 마음을 정리하는게 좋겠네요 .\n",
            "\n",
            "사용자 > 왜 마음을 정리해야하니?\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 너 왜 같은 말만 하니?\n",
            "챗봇 > 마음을 정리하는게 좋겠어요 .\n",
            "\n",
            "사용자 > 너가 똑같은 말만 하니까 슬퍼\n",
            "챗봇 > 이제 상대에게 부담되지 않는 게 좋을 것 같아요 .\n",
            "\n",
            "사용자 > 아쉽다 챗봇아\n",
            "챗봇 > 마음을 이제 빠져나오세요 .\n",
            "\n",
            "사용자 > 그래 너한테서 빠져나갈게 \n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 그래 안녕\n",
            "챗봇 > 마세요 .\n",
            "\n",
            "사용자 > 종료\n",
            "챗봇과의 대화를 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화형 인터페이스 실행\n",
        "interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_pPpetHtu-2i",
        "outputId": "f8b9d966-a1e5-4634-b1fc-012d87babcb4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 챗봇과 대화를 시작합니다. '종료'를 입력하면 대화가 종료됩니다.\n",
            "================================================================================\n",
            "사용자 > 가스비 너무 많이 나온듯\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 절약하라고 말해야 하는 거아니니\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 가난한 자의 설움\n",
            "챗봇 > 잘 생각해보세요 .\n",
            "\n",
            "사용자 > 가끔 궁금해\n",
            "챗봇 > 잘 어울리는 옷을 생각해봐요 .\n",
            "\n",
            "사용자 > 걔는 누굴 닮아서 그런 거니?\n",
            "챗봇 > 잘 어울리는 옷을 생각해봐요 .\n",
            "\n",
            "사용자 > 졸리다\n",
            "챗봇 > 잘 찾아보세요 .\n",
            "\n",
            "사용자 > 배고프다\n",
            "챗봇 > 잘 안되는게 사람 마음인가봐요 .\n",
            "\n",
            "사용자 > 고양이 동영상 보는 중\n",
            "챗봇 > 잘 필요 없어요 .\n",
            "\n",
            "사용자 > 너 바보니?\n",
            "챗봇 > 더 힘들어져요 .\n",
            "\n",
            "사용자 > 그래 알겠어\n",
            "챗봇 > 좋죠 .\n",
            "\n",
            "사용자 > 종료\n",
            "챗봇과의 대화를 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 디버깅용 함수\n",
        "def debug_evaluate(sentence):\n",
        "    # 문장 전처리\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # 전처리된 문장 확인\n",
        "    print(\"전처리된 문장:\", sentence)\n",
        "\n",
        "    # 문장 토큰화\n",
        "    encoder_input = tokenizer.encode(sentence)\n",
        "    print(\"인코더 입력 토큰:\", encoder_input)\n",
        "    print(\"인코더 입력 토큰 디코딩:\", tokenizer.decode(encoder_input))\n",
        "\n",
        "    encoder_input = [START_TOKEN_ID] + encoder_input + [END_TOKEN_ID]\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # 디코더 입력은 시작 토큰으로 시작\n",
        "    decoder_input = [START_TOKEN_ID]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    # 각 스텝의 출력 저장\n",
        "    step_outputs = []\n",
        "\n",
        "    # 자동 회귀(auto-regressive) 방식으로 출력 생성\n",
        "    for i in range(MAX_LENGTH):\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                'inputs': encoder_input,\n",
        "                'dec_inputs': output\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        # 마지막 토큰의 예측에서 다음 토큰 선택\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # 생성된 토큰 확인\n",
        "        token_id = predicted_id.numpy()[0][0]\n",
        "        token_text = tokenizer.decode([token_id]) if token_id < tokenizer.vocab_size else f\"<SPECIAL_{token_id}>\"\n",
        "        step_outputs.append((token_id, token_text))\n",
        "\n",
        "        # 종료 토큰이 예측되면 반환\n",
        "        if tf.equal(predicted_id, END_TOKEN_ID):\n",
        "            break\n",
        "\n",
        "        # 예측된 ID를 디코더 입력에 연결\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # 각 스텝 출력 확인\n",
        "    print(\"\\n생성된 토큰 시퀀스 (단계별):\")\n",
        "    for i, (token_id, token_text) in enumerate(step_outputs):\n",
        "        print(f\"스텝 {i+1}: ID={token_id}, 텍스트='{token_text}'\")\n",
        "\n",
        "    # 전체 출력 텍스트 확인\n",
        "    full_output = output[0].numpy()\n",
        "    print(\"\\n전체 출력 토큰:\", full_output)\n",
        "\n",
        "    # 출력 텍스트로 변환\n",
        "    output_text = tokenizer.decode([i for i in full_output if i < tokenizer.vocab_size])\n",
        "    print(\"최종 출력 텍스트:\", output_text)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# 테스트\n",
        "test_sentence = \"안녕하세요?\"\n",
        "debug_result = debug_evaluate(test_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1d6Qcdhp_lE",
        "outputId": "13e36195-f0de-419d-bfde-688176fdf4ea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리된 문장: 안녕하세요 ?\n",
            "인코더 입력 토큰: [2183, 2]\n",
            "인코더 입력 토큰 디코딩: 안녕하세요 ?\n",
            "\n",
            "생성된 토큰 시퀀스 (단계별):\n",
            "스텝 1: ID=10, 텍스트='잘 '\n",
            "스텝 2: ID=5471, 텍스트='하셨어요'\n",
            "스텝 3: ID=1, 텍스트=' .'\n",
            "스텝 4: ID=21835, 텍스트='<SPECIAL_21835>'\n",
            "\n",
            "전체 출력 토큰: [21834    10  5471     1]\n",
            "최종 출력 텍스트: 잘 하셨어요 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 모델 저장 및 불러오기\n",
        "\n",
        "학습이 끝난 모델을 저장하고 나중에 다시 불러올 수 있도록 함수를 구현합니다."
      ],
      "metadata": {
        "id": "zrLUjqtWo3rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 저장\n",
        "import pickle\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump({\n",
        "        'tokenizer': tokenizer,\n",
        "        'start_token': START_TOKEN_ID,\n",
        "        'end_token': END_TOKEN_ID,\n",
        "        'vocab_size': VOCAB_SIZE\n",
        "    }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print('토크나이저 저장 완료')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SLMiewL1o1bb",
        "outputId": "5c31d738-84f8-4bd9-9fda-4f5018471ad5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토크나이저 저장 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 불러오기 함수\n",
        "def load_tokenizer():\n",
        "    with open('tokenizer.pickle', 'rb') as handle:\n",
        "        data = pickle.load(handle)\n",
        "\n",
        "    return data['tokenizer'], data['start_token'], data['end_token'], data['vocab_size']\n",
        "\n",
        "# 테스트\n",
        "loaded_tokenizer, loaded_start, loaded_end, loaded_vocab_size = load_tokenizer()\n",
        "print(f'불러온 어휘 사전 크기: {loaded_vocab_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7u2j-n9Po78D",
        "outputId": "9f84a2a9-61ab-4440-83fa-0007ee04252e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불러온 어휘 사전 크기: 21836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##회고\n",
        "\n",
        "현정: 코드가 너무 길기도 하고, 오류가 너무 많아서 많이 헤맸지만! 그래서 한국어가 잘 출력되는 무언가를 완성한 듯 해서 뿌듯한 기분이 듭니다:)\n",
        "\n",
        "무엇보다 데이터의 중요성을 깨달았던 프로젝트이기도 했습니다. 애초부터 데이터 상의 Q:A 자체가 자연스러운 대화와는 먼 데이터이다보니까, 출력 결과물도 자연스럽기를 기대할 수는 없겠더라구요 ㅠㅠ 초반에는 주어가 빠진 채로 출력이 되어서 단어장 개수를 늘렸더니 조금은 문장같은 모습으로 개선이 되는 과정을 겪기도 했습니다.\n",
        "\n",
        "이후에 AI HUB 데이터를 학습 시켜봤는데, 이번에는 너무 편향된 학습이 진행되어서 여행 관련 내용을 이야기하거나 애초에 데이터 상에 있는 질문을 해야 조금은 자연스러운 답변이 나오는 것을 확인할 수 있었습니다. 가장 큰 미스는 에포크를 너무 많이 학습시킨 것 같다는 점입니다. ChatBotData 로 시도한 에포크 20이 적절하다는 걸 확인할 수 있었습니다..!\n",
        "\n",
        "여러모로 즐거운 실험이었던 것 같습니다. 다시 한번 생성형 ai의 소중함을 깨닫는 시간 ㅎㅎ\n",
        "\n",
        "지윤: 현정님의 하드캐리와 박지윤의 꼽사리  나는 스카이넷을 만들었다. 허어 ㅜㅜㅜㅜㅜㅜㅜ 너무 어려움...\n",
        "\n"
      ],
      "metadata": {
        "id": "x4abA8LjS6ZM"
      }
    }
  ]
}